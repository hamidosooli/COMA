  0%|          | 0/100000 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/Users/hamidosooli/PycharmProjects/MARL/COMA/training.py", line 123, in <module>
    actors_grad_norm = alg.optimize_actors()
  File "/Users/hamidosooli/PycharmProjects/MARL/COMA/alg.py", line 80, in optimize_actors
    self.actors_loss.backward()
  File "/Users/hamidosooli/PycharmProjects/pythonProject1/venv/lib/python3.9/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/Users/hamidosooli/PycharmProjects/pythonProject1/venv/lib/python3.9/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/Users/hamidosooli/PycharmProjects/pythonProject1/venv/lib/python3.9/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
